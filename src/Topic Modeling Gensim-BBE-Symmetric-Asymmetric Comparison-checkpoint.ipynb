{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gg0j-uNH58oL"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKW-GCGZ4SUc",
    "outputId": "37642d2e-4c81-40c8-c0d2-b35f3d0b7536"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15138\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from gensim.models import CoherenceModel\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe = pd.read_csv('t_bbe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1euX5RAvqqIy",
    "outputId": "ed11c3ce-9408-4d4f-a851-856bd47f1c0b"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WIbwSbssqqI0"
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NrJNFw5dqqI3"
   },
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values_a(corpus, dictionary, k, a, b):\n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QIcnZhm9qqI4"
   },
   "outputs": [],
   "source": [
    "def _get_best_num_topics(topics_range, alpha_lst, beta_lst, corpus):\n",
    "    corpus_sets = [corpus]\n",
    "    # maximum coherence score (value)\n",
    "    max_cv = 0.0\n",
    "    alpha = 0.0\n",
    "    beta = 0.0\n",
    "    num_topics = 2\n",
    "\n",
    "    # iterate through the corus\n",
    "    for i in tqdm(range(len(corpus_sets))):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha_lst:\n",
    "                # iterare through beta values\n",
    "                for b in beta_lst:\n",
    "                    # compute coherence value\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], \n",
    "                                                dictionary=id2word, k=k, a=a, b=b)\n",
    "                    if cv > max_cv:\n",
    "                        max_cv = cv\n",
    "                        alpha = a\n",
    "                        beta = b\n",
    "                        num_topics = k\n",
    "\n",
    "    return alpha, beta, num_topics, max_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_best_num_topics_a(topics_range, alpha_lst, beta_lst, corpus):\n",
    "    corpus_sets = [corpus]\n",
    "    # maximum coherence score (value)\n",
    "    max_cv = 0.0\n",
    "    alpha = 0.0\n",
    "    beta = 0.0\n",
    "    num_topics = 2\n",
    "\n",
    "    # iterate through the corus\n",
    "    for i in tqdm(range(len(corpus_sets))):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha_lst:\n",
    "                # iterare through beta values\n",
    "                for b in beta_lst:\n",
    "                    # compute coherence value\n",
    "                    cv = compute_coherence_values_a(corpus=corpus_sets[i], \n",
    "                                                dictionary=id2word, k=k, a=a, b=b)\n",
    "                    if cv > max_cv:\n",
    "                        max_cv = cv\n",
    "                        alpha = a\n",
    "                        beta = b\n",
    "                        num_topics = k\n",
    "\n",
    "    return alpha, beta, num_topics, max_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/#6.-What-is-the-Dominant-topic-and-its-percentage-contribution-in-each-document\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "uI-nkzSrqqIw",
    "outputId": "b05bdb65-eeea-4513-89dd-ded148b96183"
   },
   "outputs": [],
   "source": [
    "# Create list of book documents and put them in a dataframe\n",
    "booksList = []\n",
    "for book in bbe.groupby('b')['t']:\n",
    "    booksList.append(' '.join(book[1].tolist()))\n",
    "books = pd.DataFrame(booksList, columns =['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\.\n",
      "C:\\Users\\15138\\AppData\\Local\\Temp/ipykernel_5916/1686976997.py:2: DeprecationWarning: invalid escape sequence \\.\n",
      "  books['t_processed'] = books['t'].map(lambda x: re.sub('[,\\.!?]', '', x))\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "books['t_processed'] = books['t'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "books['t_processed'] = books['t_processed'].map(lambda x: x.lower())\n",
    "\n",
    "#### Tokenize words and further clean-up text\n",
    "\n",
    "data = books.t_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#### Creating Bigram and Trigram Models\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "\n",
    "bigram = gensim.models.Phrases(data_words, min_count=3, threshold=5)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=5)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "#list of stopwords\n",
    "with open('stop_words_lst.txt') as f:\n",
    "    standard_stop_words = f.read().splitlines()\n",
    "with open('bible_stop_words_lst.txt') as f:\n",
    "    bible_stop_words = f.read().splitlines()\n",
    "stop_words = standard_stop_words + bible_stop_words\n",
    "### Call the functions above\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "### Data transformation: Corpus and Dictionary\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Average Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [38:47<00:00, 2327.79s/it]\n",
      "100%|██████████| 1/1 [37:29<00:00, 2249.28s/it]\n",
      "100%|██████████| 1/1 [37:31<00:00, 2251.45s/it]\n",
      "100%|██████████| 1/1 [37:40<00:00, 2260.50s/it]\n",
      "100%|██████████| 1/1 [37:35<00:00, 2255.36s/it]\n",
      "100%|██████████| 1/1 [37:32<00:00, 2252.70s/it]\n",
      "100%|██████████| 1/1 [37:32<00:00, 2252.64s/it]\n",
      "100%|██████████| 1/1 [37:31<00:00, 2251.10s/it]\n",
      "100%|██████████| 1/1 [37:30<00:00, 2250.94s/it]\n",
      "100%|██████████| 1/1 [37:29<00:00, 2249.90s/it]\n",
      "100%|██████████| 1/1 [37:34<00:00, 2254.51s/it]\n",
      "100%|██████████| 1/1 [37:35<00:00, 2255.10s/it]\n",
      "100%|██████████| 1/1 [37:31<00:00, 2251.40s/it]\n",
      "100%|██████████| 1/1 [37:27<00:00, 2247.23s/it]\n",
      "100%|██████████| 1/1 [37:34<00:00, 2254.50s/it]\n",
      "100%|██████████| 1/1 [37:32<00:00, 2252.99s/it]\n",
      "100%|██████████| 1/1 [37:34<00:00, 2254.63s/it]\n",
      "100%|██████████| 1/1 [37:42<00:00, 2262.27s/it]\n",
      "100%|██████████| 1/1 [37:40<00:00, 2260.57s/it]\n",
      "100%|██████████| 1/1 [37:38<00:00, 2258.04s/it]\n",
      "100%|██████████| 1/1 [37:17<00:00, 2237.13s/it]\n",
      "100%|██████████| 1/1 [37:24<00:00, 2244.92s/it]\n",
      "100%|██████████| 1/1 [37:16<00:00, 2236.82s/it]\n",
      "100%|██████████| 1/1 [37:13<00:00, 2233.54s/it]\n",
      "100%|██████████| 1/1 [37:25<00:00, 2245.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg alpha:  1.1028\n",
      "Avg beta:  1.82\n",
      "Avg Number of topics:  6.4\n",
      "Avg Optimal Coherence Value:  0.40937109728419385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step_size = 2\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# List of alpha parameters\n",
    "alpha_lst =  [0.01, 0.5, 1.0, 2.0, 3.0]\n",
    "\n",
    "# List of beta parameters\n",
    "beta_lst = [0.01, 0.5, 1, 1.0, 2.0, 3.0]\n",
    "\n",
    "avg_alpha = 0.0\n",
    "avg_beta = 0.0\n",
    "avg_num_topics = 0\n",
    "avg_cv = 0.0\n",
    "num_runs = 25\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # Call to the method above\n",
    "    alpha, beta, num_topics, max_cv = _get_best_num_topics(topics_range, alpha_lst, beta_lst, corpus)\n",
    "    avg_alpha += alpha\n",
    "    avg_beta += beta\n",
    "    avg_num_topics += num_topics\n",
    "    avg_cv += max_cv\n",
    "\n",
    "avg_alpha /= num_runs\n",
    "avg_beta /= num_runs\n",
    "avg_num_topics /= num_runs\n",
    "avg_cv /= num_runs\n",
    "\n",
    "print(\"Avg alpha: \", avg_alpha)\n",
    "print(\"Avg beta: \", avg_beta)\n",
    "print(\"Avg Number of topics: \", avg_num_topics)\n",
    "print(\"Avg Optimal Coherence Value: \", avg_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric Average Model Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [04:50<00:00, 290.13s/it]\n",
      "100%|██████████| 1/1 [04:52<00:00, 292.21s/it]\n",
      "100%|██████████| 1/1 [04:51<00:00, 291.45s/it]\n",
      "100%|██████████| 1/1 [04:51<00:00, 291.61s/it]\n",
      "100%|██████████| 1/1 [04:51<00:00, 291.40s/it]\n",
      "100%|██████████| 1/1 [04:51<00:00, 291.72s/it]\n",
      "100%|██████████| 1/1 [04:53<00:00, 293.72s/it]\n",
      "100%|██████████| 1/1 [04:50<00:00, 290.38s/it]\n",
      "100%|██████████| 1/1 [04:49<00:00, 289.57s/it]\n",
      "100%|██████████| 1/1 [04:50<00:00, 290.62s/it]\n",
      "100%|██████████| 1/1 [04:46<00:00, 286.17s/it]\n",
      "100%|██████████| 1/1 [04:51<00:00, 291.69s/it]\n",
      "100%|██████████| 1/1 [04:49<00:00, 289.07s/it]\n",
      "100%|██████████| 1/1 [04:50<00:00, 290.64s/it]\n",
      "100%|██████████| 1/1 [04:47<00:00, 287.64s/it]\n",
      "100%|██████████| 1/1 [04:52<00:00, 292.64s/it]\n",
      "100%|██████████| 1/1 [04:51<00:00, 291.09s/it]\n",
      "100%|██████████| 1/1 [04:49<00:00, 289.95s/it]\n",
      "100%|██████████| 1/1 [04:52<00:00, 292.44s/it]\n",
      "100%|██████████| 1/1 [04:51<00:00, 291.83s/it]\n",
      "100%|██████████| 1/1 [05:32<00:00, 332.55s/it]\n",
      "100%|██████████| 1/1 [04:53<00:00, 293.93s/it]\n",
      "100%|██████████| 1/1 [04:56<00:00, 296.72s/it]\n",
      "100%|██████████| 1/1 [04:54<00:00, 294.01s/it]\n",
      "100%|██████████| 1/1 [04:52<00:00, 292.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg beta:  1.98\n",
      "Avg Number of topics:  5.76\n",
      "Avg Optimal Coherence Value:  0.40336945567563354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Topics range\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step_size = 2\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# List of alpha parameters\n",
    "alpha_lst =  ['auto']\n",
    "\n",
    "# List of beta parameters\n",
    "beta_lst = [0.01, 0.5, 1, 1.0, 2.0, 3.0]\n",
    "\n",
    "avg_beta = 0.0\n",
    "avg_num_topics = 0\n",
    "avg_cv = 0.0\n",
    "num_runs = 25\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # Call to the method above\n",
    "    alpha, beta, num_topics, max_cv = _get_best_num_topics_a(topics_range, alpha_lst, beta_lst, corpus)\n",
    "    avg_beta += beta\n",
    "    avg_num_topics += num_topics\n",
    "    avg_cv += max_cv\n",
    "    \n",
    "avg_beta /= num_runs\n",
    "avg_num_topics /= num_runs\n",
    "avg_cv /= num_runs\n",
    "\n",
    "print(\"Avg beta: \", avg_beta)\n",
    "print(\"Avg Number of topics: \", avg_num_topics)\n",
    "print(\"Avg Optimal Coherence Value: \", avg_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "LDA-Topic-Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
